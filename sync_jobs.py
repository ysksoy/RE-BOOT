import json
import os
import csv
from dotenv import load_dotenv
from db_client_template import DBClient

# Load environment variables (expecting .env in the same dir)
load_dotenv()

OUTPUT_FILE = "src/data/jobs.json"
STATION_CSV_FILE = "station20251211free.csv"

# --- Configuration & Constants (Ported from merge_jobs.py) ---
FILTER_TARGET_SOURCES = ["Indeed", "Kyujinbox"]
REQUIRED_KEYWORDS = ["æœªçµŒé¨“", "åˆå¿ƒè€…"]
NG_KEYWORDS = [
    "è»½ä½œæ¥­", "å€‰åº«", "ä»•åˆ†ã‘", "ãƒ”ãƒƒã‚­ãƒ³ã‚°", "æ¢±åŒ…", 
    "ãƒ›ãƒ¼ãƒ«", "ã‚­ãƒƒãƒãƒ³", "èª¿ç†", "æ¸…æƒ", "è­¦å‚™", 
    "ã‚³ãƒ³ãƒ“ãƒ‹", "ãƒ¬ã‚¸", "ãƒ‰ãƒ©ã‚¤ãƒãƒ¼", "é…é€", "é…é”", 
    "å·¥å ´", "è£½é€ ", "ãƒ©ã‚¤ãƒ³ä½œæ¥­", "ãƒ‘ãƒãƒ³ã‚³", "ã‚«ãƒ©ã‚ªã‚±",
    "å¼•è¶Š", "æ–½å·¥ç®¡ç†", "çœ‹è­·å¸«", "è–¬å‰¤å¸«", "ä»‹è­·"
]

PREF_CODE_MAP = {
    1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
    8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
    15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ",
    21: "å²é˜œçœŒ", 22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ",
    28: "å…µåº«çœŒ", 29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ",
    35: "å±±å£çœŒ", 36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ",
    41: "ä½è³€çœŒ", 42: "é•·å´çœŒ", 43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
}

PREF_PRIORITY = [
    "æ±äº¬éƒ½", "å¤§é˜ªåºœ", "ç¥å¥ˆå·çœŒ", "äº¬éƒ½åºœ", "æ„›çŸ¥çœŒ", "ç¦å²¡çœŒ", "åŒ—æµ·é“", "å…µåº«çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ"
]

STATION_PREF_MAP = {}

# --- Geographic Helpers ---

def load_station_data(csv_path):
    global STATION_PREF_MAP
    try:
        if not os.path.exists(csv_path):
             # Try absolute path if relative fails (assuming script is in R-website root)
             csv_path = os.path.join(os.path.dirname(__file__), csv_path)

        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            count = 0
            for row in reader:
                name = row['station_name']
                pref_cd = int(row['pref_cd'])
                pref_name = PREF_CODE_MAP.get(pref_cd)
                if pref_name:
                    if name not in STATION_PREF_MAP:
                        STATION_PREF_MAP[name] = set()
                    STATION_PREF_MAP[name].add(pref_name)
                    count += 1
            print(f"âœ… Loaded {count} stations from CSV.")
    except Exception as e:
        print(f"âš ï¸ Station CSV loading error: {e}")
        STATION_PREF_MAP = {}

def detect_prefecture(location):
    if not location or location == "N/A":
        return None
    
    # 1. Direct Match
    for code, pref in PREF_CODE_MAP.items():
        if pref in location:
            return pref
            
    # 2. Key/Station Match
    possible_prefs = set()
    for station, prefs in STATION_PREF_MAP.items():
        if len(station) < 2: 
            continue
        if station in location:
            possible_prefs.update(prefs)

    if not possible_prefs:
        return None
        
    for p in PREF_PRIORITY:
        if p in possible_prefs:
            return p
            
    return list(possible_prefs)[0]

# --- Filtering Logic ---

def is_valid_job(job):
    # Only filter specific sources if needed (Currently filtering Indeed/Kyujinbox based on keyword rules)
    # The requirement is to generally apply these valid filters to ensure quality.
    
    source = job.get('site_name') or job.get('source')
    # If the job explicitly comes from a source we want to be strict about:
    if source in FILTER_TARGET_SOURCES:
        pass # proceed to check
    else:
        # For Infra/ZeroOne, assume they are generally valid or apply same rules?
        # Let's apply the 'NG Keyword' rule to ALL, but 'Required Keyword' only to generic search engines.
        # But for now, sticking to merge_jobs.py logic:
        # "if job.get('source') not in FILTER_TARGET_SOURCES: return True"
        pass

    if source not in FILTER_TARGET_SOURCES:
        return True

    title = str(job.get('title', ''))
    summary = str(job.get('summary', ''))
    full_text = (title + summary).replace("\n", "").replace(" ", "")
    
    # 1. Required Keywords
    if not any(req in full_text for req in REQUIRED_KEYWORDS):
        return False
        
    # 2. NG Keywords
    if any(ng in full_text for ng in NG_KEYWORDS):
        return False
        
    return True

# --- Main Sync Logic ---

def main():
    print("ğŸš€ Starting Sync Jobs from Supabase...")
    
    # Initialize DB
    try:
        db = DBClient()
    except Exception as e:
        print(f"âŒ Failed to init DB: {e}")
        return

    # 1. Cleanup Old Jobs (Older than 30 days)
    print("ğŸ§¹ Cleaning up old jobs...")
    deleted_count = db.delete_old_jobs(days=30)
    print(f"   -> Deleted {deleted_count} old jobs.")

    # 2. Load Station Data for normalization
    load_station_data(STATION_CSV_FILE)

    # 3. Fetch All Jobs (Sorted by Created At DESC)
    print("ğŸ“¥ Fetching latest jobs from DB...")
    raw_jobs = db.fetch_all_jobs()
    print(f"   -> Fetched {len(raw_jobs)} jobs.")

    # 4. Process & Format
    valid_jobs_list = []
    
    for job in raw_jobs:
        # Standardize 'source' key (DB uses 'site_name', Frontend expects 'source' or handles both?)
        # page.tsx defines type Job with optional 'source'.
        # manual says DB has 'site_name'.
        job['source'] = job.get('site_name')
        
        # Filter
        if is_valid_job(job):
            # Normalize Location
            loc = job.get('location', '')
            detect_pref = detect_prefecture(loc)
            
            if detect_pref:
                job['prefecture'] = detect_pref
                if detect_pref not in loc:
                    job['location'] = f"{detect_pref} {loc}"
            
            # Map 'link' to 'url' because frontend might use 'link' (Job type says 'link: string | null')
            if not job.get('link') and job.get('url'):
                job['link'] = job['url']

            valid_jobs_list.append(job)

    print(f"âœ… Processing complete. {len(valid_jobs_list)} jobs valid after filtering.")

    # 5. Write to JSON
    # Ensure directory exists
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(valid_jobs_list, f, indent=2, ensure_ascii=False)
        
    print(f"ğŸ‰ Saved to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
