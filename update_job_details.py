import json
import asyncio
import random
import os
from openai import OpenAI
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# Load environment variables
load_dotenv()

DATA_FILE = "src/data/jobs.json"
TARGET_SOURCE = "Infra"

# OpenAI Client Setup
api_key = os.environ.get("OPENAI_API_KEY")

client = OpenAI(api_key=api_key) if api_key else None

if not api_key:
    print("âš ï¸  OPENAI_API_KEY is not set in .env. AI recommendations will be skipped.")

def generate_ai_recommendation(title, summary):
    if not client or not summary:
        return None

    try:
        response = client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {
                    "role": "system", 
                    "content": """ã‚ãªãŸã¯æœªçµŒé¨“è€…å‘ã‘æ±‚äººã‚µã‚¤ãƒˆã€ŒRE:BOOTã€ã®ç·¨é›†éƒ¨ã§ã™ã€‚
æ±‚äººã®ã€Œä¸€ç•ªã®ã‚¦ãƒªï¼ˆé«˜æ™‚çµ¦ã€ãƒ•ãƒ«ãƒªãƒ¢ãƒ¼ãƒˆã€æœ‰åä¼æ¥­ã€ç‰¹å®šã®ã‚¹ã‚­ãƒ«ç¿’å¾—ãªã©ï¼‰ã€ã‚’è¦‹ã¤ã‘å‡ºã—ã€ãã‚Œã‚’å¼·èª¿ã—ãŸãŠã™ã™ã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚’100ã€œ150æ–‡å­—ç¨‹åº¦ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

# é‡è¦ãƒ«ãƒ¼ãƒ«
- ã€Œæˆé•·ã§ãã‚‹ã€ã€Œã‚„ã‚ŠãŒã„ãŒã‚ã‚‹ã€ã¨ã„ã£ãŸ**ã‚ã‚ŠããŸã‚Šãªè¡¨ç¾ã¯ç¦æ­¢**ã§ã™ã€‚å…·ä½“çš„ã«ä½•ãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯æœªçµŒé¨“ã®å¤§å­¦ç”Ÿã§ã™ã€‚å½¼ã‚‰ã«ã¨ã£ã¦é­…åŠ›çš„ãªãƒ¡ãƒªãƒƒãƒˆï¼ˆç¨¼ã’ã‚‹ã€å°±æ´»ã«æœ‰åˆ©ãªã©ï¼‰ã‚’å…·ä½“çš„ã«è¨´æ±‚ã—ã¦ãã ã•ã„ã€‚
- èª­ã¿æ‰‹ã®ç›®ã‚’å¼•ãã‚ˆã†ãªã€å°‘ã—ã‚¨ãƒƒã‚¸ã®åŠ¹ã„ãŸã‚­ãƒ£ãƒƒãƒãƒ¼ãªæ›¸ãå‡ºã—ã«ã—ã¦ãã ã•ã„ã€‚
- ä¸å¯§èªï¼ˆã€œã§ã™ã€ã€œã¾ã™ï¼‰ã§æ›¸ãã¾ã™ãŒã€å …è‹¦ã—ããªã‚‰ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€Œã€œãªã‚‰ä»Šã™ãå¿œå‹Ÿï¼ã€ã€Œã€œã—ãŸã„äººã«ãŠã™ã™ã‚ï¼ã€ãªã©ã§è¡Œå‹•ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚"""
                },
                {"role": "user", "content": f"æ±‚äººã‚¿ã‚¤ãƒˆãƒ«: {title}\n\næ¦‚è¦:\n{summary[:1000]}"}
            ],
            max_completion_tokens=8000  # 4000ã§ã‚‚è¶³ã‚Šãªã„ã‚±ãƒ¼ã‚¹ãŒã‚ã£ãŸãŸã‚å€å¢—
        )

        if response.choices and len(response.choices) > 0:
            content = response.choices[0].message.content
            if content:
                return content.strip()
            return None
        return None
    except Exception as e:
        print(f"âš ï¸ OpenAI API Error: {e}")
        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
        if hasattr(e, 'response'): 
            pass # Removed debug print
        return None

async def scrape_job_detail(page, url):
    # Removed debug print: print(f"ğŸ” Visiting: {url}")
    try:
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
        await asyncio.sleep(random.uniform(2, 4)) # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“

        # HTMLè§£æ
        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")

        details = {}

        # JSON-LDã®è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå…±é€šï¼‰
        json_ld_elem = soup.find("script", type="application/ld+json")
        if json_ld_elem:
            try:
                data = json.loads(json_ld_elem.string)
                if isinstance(data, list):
                    data = data[0] if data else {}
                
                if data.get("description"):
                    raw_desc = data["description"]
                    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®çµ±ä¸€
                    clean_desc = raw_desc.replace("<br>", "\n").replace("<br />", "\n").replace("<br/>", "\n")
                    # HTMLã‚¿ã‚°é™¤å» & ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                    soup_desc = BeautifulSoup(clean_desc, "html.parser")
                    details["summary"] = soup_desc.get_text(separator="\n", strip=True)

            except json.JSONDecodeError:
                print("âš ï¸ Failed to parse JSON-LD")

        # 2. ç”»åƒ (Metaã‚¿ã‚°ã‹ã‚‰å–å¾—)
        og_image = soup.find("meta", property="og:image")
        if og_image and og_image.get("content"):
            details["image_url"] = og_image["content"]

        # JSON-LDã§å–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not details.get("summary"):
             summary_elem = soup.select_one(".job-description") or soup.select_one(".post-content")
             if summary_elem:
                details["summary"] = summary_elem.get_text(strip=True)[:600]
        
        return details

    except Exception as e:
        print(f"âš ï¸ Error scraping {url}: {e}")
        return None

async def main():
    print("ğŸš€ Starting detailed scraping & AI generation...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            jobs = json.load(f)
    except FileNotFoundError:
        print("âŒ Jobs data file not found.")
        return

    # Reverted force-update logic: only process jobs from TARGET_SOURCE
    target_jobs = [
        j for j in jobs 
        if j.get("source") == TARGET_SOURCE and (not j.get("summary") or not j.get("recommendation"))
    ]
    
    print(f"ğŸ“‹ Found {len(target_jobs)} jobs to process.")
    if len(target_jobs) == 0:
        print("âœ… No jobs need updating.")
        return

    # 2. å‡¦ç†å®Ÿè¡Œ
    updated_count = 0
    BATCH_SIZE = 100
    
    # Playwrightèµ·å‹•ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
    # summaryãŒãªã„ã‚¸ãƒ§ãƒ–ãŒ1ã¤ã§ã‚‚ã‚ã‚‹å ´åˆã®ã¿ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
    needs_scraping = any(not j.get("summary") for j in target_jobs[:BATCH_SIZE])
    
    browser = None
    page = None
    playwright = None

    if needs_scraping:
        playwright = await async_playwright().start()
        browser = await playwright.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

    try:
        for i, job in enumerate(target_jobs[:BATCH_SIZE]):
            print(f"[{i+1}/{min(len(target_jobs), BATCH_SIZE)}] Processing: {job['title']}...")
            job_updated = False
            
            # 1. Scraping (if summary missing)
            if not job.get("summary") and page:
                details = await scrape_job_detail(page, job["link"])
                if details:
                    if details.get("summary"):
                        job["summary"] = details["summary"]
                        job_updated = True
                    if details.get("image_url"):
                        job["image_url"] = details["image_url"]
                        job_updated = True
            
            # 2. AI Recommendation (if summary exists but recommendation missing)
            if job.get("summary") and not job.get("recommendation") and api_key:
                print("  ğŸ¤– Generating AI recommendation...")
                rec_text = generate_ai_recommendation(job["title"], job["summary"])
                if rec_text:
                    job["recommendation"] = rec_text
                    print("  âœ¨ Recommendation generated.")
                    job_updated = True
                else:
                    print("  âš ï¸ AI Generation failed.")
            
            if job_updated:
                updated_count += 1

    finally:
        if browser:
            await browser.close()
        if playwright:
            await playwright.stop()

    # 3. ä¿å­˜
    if updated_count > 0:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Saved {updated_count} jobs with new details to {DATA_FILE}")
    else:
        print("No changes saved.")

if __name__ == "__main__":
    asyncio.run(main())
