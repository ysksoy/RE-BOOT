
import json
import asyncio
import random
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

DATA_FILE = "src/data/jobs.json"
TARGET_SOURCE = "Infra"

async def scrape_job_detail(page, url):
    print(f"ğŸ” Visiting: {url}")
    try:
        await page.goto(url, timeout=60000, wait_until="domcontentloaded")
        await asyncio.sleep(random.uniform(2, 4)) # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“

        # HTMLè§£æ
        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")

        details = {}

        # JSON-LDã®è§£æãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå…±é€šï¼‰
        json_ld_elem = soup.find("script", type="application/ld+json")
        if json_ld_elem:
            try:
                data = json.loads(json_ld_elem.string)
                if isinstance(data, list):
                    data = data[0] if data else {}
                
                if data.get("description"):
                    raw_desc = data["description"]
                    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®çµ±ä¸€
                    clean_desc = raw_desc.replace("<br>", "\n").replace("<br />", "\n").replace("<br/>", "\n")
                    # HTMLã‚¿ã‚°é™¤å» & ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                    soup_desc = BeautifulSoup(clean_desc, "html.parser")
                    details["summary"] = soup_desc.get_text(separator="\n", strip=True)

            except json.JSONDecodeError:
                print("âš ï¸ Failed to parse JSON-LD")

        # 2. ç”»åƒ (Metaã‚¿ã‚°ã‹ã‚‰å–å¾—)
        og_image = soup.find("meta", property="og:image")
        if og_image and og_image.get("content"):
            details["image_url"] = og_image["content"]

        # JSON-LDã§å–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not details.get("summary"):
             summary_elem = soup.select_one(".job-description") or soup.select_one(".post-content")
             if summary_elem:
                details["summary"] = summary_elem.get_text(strip=True)[:600]

        # ç‰¹å¾´ã‚¿ã‚°ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
        # ç¾çŠ¶ã®HTMLæ§‹é€ ã‹ã‚‰ã¯æ˜ç¢ºãªãƒªã‚¹ãƒˆãŒè¦‹å½“ãŸã‚‰ãªã„ãŸã‚ã€å®Ÿè£…ä¿ç•™
        
        return details

    except Exception as e:
        print(f"âš ï¸ Error scraping {url}: {e}")
        return None

async def main():
    print("ğŸš€ Starting detailed scraping for jobs...")
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            jobs = json.load(f)
    except FileNotFoundError:
        print("âŒ Jobs data file not found.")
        return

    TARGET_SOURCES = ["Infra", "ZeroOne"]

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾è±¡ã‚½ãƒ¼ã‚¹ã®æ±‚äººã‚’æŠ½å‡º (ã¾ã summaryãŒãªã„ã‚‚ã®ã€ã¾ãŸã¯å†åº¦å–å¾—ã—ãŸã„å ´åˆã¯æ¡ä»¶ã‚’ç·©å’Œ)
    # ã“ã“ã§ã¯ã€ŒsummaryãŒãªã„ã€æ¡ä»¶ã‚’æ®‹ã—ã¾ã™ãŒã€syncç›´å¾Œã®æ–°è¦jsonã§ã‚ã‚Œã°å…¨ã¦summaryã¯ãªã„ã¯ãšã§ã™ã€‚
    target_jobs = [j for j in jobs if j.get("source") in TARGET_SOURCES and not j.get("summary")]
    
    print(f"ğŸ“‹ Found {len(target_jobs)} jobs to update.")
    if len(target_jobs) == 0:
        print("âœ… No jobs need updating.")
        return

    # 2. ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        updated_count = 0
        
        # æœ¬ç•ªå®Ÿè¡Œç”¨ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‹¡å¤§
        BATCH_SIZE = 100  
        
        for i, job in enumerate(target_jobs[:BATCH_SIZE]):
            print(f"[{i+1}/{min(len(target_jobs), BATCH_SIZE)}] Processing: {job['title']}...")
            
            details = await scrape_job_detail(page, job["link"])
            
            if details:
                # å…ƒã®ãƒªã‚¹ãƒˆå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥æ›´æ–°
                if details.get("summary"):
                    job["summary"] = details["summary"]
                if details.get("image_url"):
                    job["image_url"] = details["image_url"]
                if details.get("features"):
                    job["features"] = details["features"]
                
                updated_count += 1
                print("  âœ… Updated details.")
            else:
                print("  âš ï¸ Failed to get details.")

        await browser.close()

    # 3. ä¿å­˜
    if updated_count > 0:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            # å…ƒã®jobsãƒªã‚¹ãƒˆï¼ˆæ›´æ–°ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å«ã‚€ï¼‰ã‚’ä¿å­˜
            json.dump(jobs, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ Saved {updated_count} jobs with new details to {DATA_FILE}")
    else:
        print("No changes to save.")

if __name__ == "__main__":
    asyncio.run(main())
