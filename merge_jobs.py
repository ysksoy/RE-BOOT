import json
import glob
import os
import hashlib
from datetime import datetime

# å„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
PROJECT_PATHS = {
    "Infra": "../infra-scraping/output/*.json",
    "ZeroOne": "../zeroone-scraping/output/*.json",
    "Indeed": "../indeed-scraping/output/*.json",
    "Kyujinbox": "../kyujin-scraping/output/*.json"
}

OUTPUT_FILE = "src/data/jobs.json"

def get_latest_file(pattern):
    files = glob.glob(pattern)
    if not files:
        return None
    # ä½œæˆæ—¥æ™‚ï¼ˆã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«åã®æ—¥ä»˜ï¼‰ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã‚’å–å¾—
    return max(files, key=os.path.getctime)

# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
FILTER_TARGET_SOURCES = ["Indeed", "Kyujinbox"]

# å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°OKï¼‰
REQUIRED_KEYWORDS = ["æœªçµŒé¨“", "åˆå¿ƒè€…"]

# NGã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã„ãšã‚Œã‹ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°é™¤å¤–ï¼‰
NG_KEYWORDS = [
    "è»½ä½œæ¥­", "å€‰åº«", "ä»•åˆ†ã‘", "ãƒ”ãƒƒã‚­ãƒ³ã‚°", "æ¢±åŒ…", 
    "ãƒ›ãƒ¼ãƒ«", "ã‚­ãƒƒãƒãƒ³", "èª¿ç†", "æ¸…æƒ", "è­¦å‚™", 
    "ã‚³ãƒ³ãƒ“ãƒ‹", "ãƒ¬ã‚¸", "ãƒ‰ãƒ©ã‚¤ãƒãƒ¼", "é…é€", "é…é”", 
    "å·¥å ´", "è£½é€ ", "ãƒ©ã‚¤ãƒ³ä½œæ¥­", "ãƒ‘ãƒãƒ³ã‚³", "ã‚«ãƒ©ã‚ªã‚±",
    "å¼•è¶Š", "æ–½å·¥ç®¡ç†", "çœ‹è­·å¸«", "è–¬å‰¤å¸«", "ä»‹è­·"
]

import csv

# JISéƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ (1-47)
PREF_CODE_MAP = {
    1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
    8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
    15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ",
    21: "å²é˜œçœŒ", 22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ",
    28: "å…µåº«çœŒ", 29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ",
    35: "å±±å£çœŒ", 36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ",
    41: "ä½è³€çœŒ", 42: "é•·å´çœŒ", 43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
}

# å„ªå…ˆé †ä½ï¼ˆåŒåã®é§…ãŒã‚ã‚‹å ´åˆã€ã“ã®é †åºã§åˆ¤å®šã™ã‚‹ï¼‰
PREF_PRIORITY = [
    "æ±äº¬éƒ½", "å¤§é˜ªåºœ", "ç¥å¥ˆå·çœŒ", "äº¬éƒ½åºœ", "æ„›çŸ¥çœŒ", "ç¦å²¡çœŒ", "åŒ—æµ·é“", "å…µåº«çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ"
]

STATION_PREF_MAP = {}

def load_station_data(csv_path="station20251211free.csv"):
    """
    CSVã‹ã‚‰é§…ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€{é§…å: [éƒ½é“åºœçœŒãƒªã‚¹ãƒˆ]} ã®ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹
    """
    global STATION_PREF_MAP
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            count = 0
            for row in reader:
                name = row['station_name']
                pref_cd = int(row['pref_cd'])
                pref_name = PREF_CODE_MAP.get(pref_cd)
                
                if pref_name:
                    if name not in STATION_PREF_MAP:
                        STATION_PREF_MAP[name] = set()
                    STATION_PREF_MAP[name].add(pref_name)
                    count += 1
            print(f"âœ… Loaded {count} stations from CSV.")
    except FileNotFoundError:
        print("âš ï¸ Station CSV not found. Using fallback detection.")
        STATION_PREF_MAP = {}

def detect_prefecture(location):
    if not location or location == "N/A":
        return None
        
    # 1. ç›´æ¥éƒ½é“åºœçœŒåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆãƒ»æœ€å¼·ï¼‰
    for code, pref in PREF_CODE_MAP.items():
        if pref in location:
            return pref
            
    # 2. é§…åã‹ã‚‰ã®é€†å¼•ã
    # locæ–‡å­—åˆ—ã«å«ã¾ã‚Œã‚‹é§…åã‚’å…¨ã¦æ¢ã—ã€å€™è£œã®éƒ½é“åºœçœŒã‚’æŒ™ã’ã‚‹
    candidates = []
    
    # é§…åã®æ¤œç´¢ï¼ˆé•·ã„é †ã«ãƒãƒƒãƒã•ã›ã‚‹ã®ãŒç†æƒ³ã ãŒã€ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ï¼‰
    # æ–‡å­—åˆ—æ¤œç´¢ã¯é‡ã„ã®ã§ã€locationå†…ã®å˜èªã‚’åˆ‡ã‚Šå‡ºã™ã‹ã€
    # é€†ã«locationã«å¯¾ã—ã¦é§…åãƒªã‚¹ãƒˆã‚’èµ°æŸ»ã™ã‚‹
    # ä»Šå›ã¯ç²¾åº¦é‡è¦–ã§ã€STATION_PREF_MAPã®ã‚­ãƒ¼ã‚’èµ°æŸ»ã™ã‚‹ï¼ˆå°‘ã—é…ã„ã‹ã‚‚ã ãŒã€ä»¶æ•°å°‘ãªã„ã®ã§OKï¼‰
    
    # é«˜é€ŸåŒ–ã®ãŸã‚ã€locationã«å«ã¾ã‚Œãã†ãªæ–‡å­—æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ãŒã€
    # ã‚·ãƒ³ãƒ—ãƒ«ã«ã€Œ3æ–‡å­—ä»¥ä¸Šã®é§…åã€ã‚ã‚‹ã„ã¯ã€Œã€‡ã€‡é§…ã€ã¨ã„ã†å½¢å¼ã‚’æ¢ã™ã®ãŒè‰¯ã„
    # ã—ã‹ã—ã€Œäº”åç”°ã€ã®ã‚ˆã†ã«ã€Œé§…ã€ãŒã¤ã‹ãªã„å ´åˆã‚‚ã‚ã‚‹ã€‚
    
    # æˆ¦ç•¥: locationã«å«ã¾ã‚Œã‚‹æ–‡å­—åˆ—ã¨é§…åã®ãƒãƒƒãƒãƒ³ã‚°
    possible_prefs = set()
    
    # locationãŒé•·ã„å ´åˆã€å…¨é§…ãƒ«ãƒ¼ãƒ—ã¯å³ã—ã„ã‹ï¼Ÿ
    # -> é§…ãƒ‡ãƒ¼ã‚¿ã¯1ä¸‡ä»¶ç¨‹åº¦ã€‚100ä»¶ã®æ±‚äººã«å¯¾ã—ã¦ãªã‚‰ 100 x 10000 = 100ä¸‡å›æ¯”è¼ƒã€‚Pythonãªã‚‰æ•°ç§’ã€‚
    
    for station, prefs in STATION_PREF_MAP.items():
        # èª¤çˆ†é˜²æ­¢: 2æ–‡å­—ä»¥ä¸Šã®é§…åã«é™å®šã€ã‹ã¤ä¸€èˆ¬çš„ãªå˜èªï¼ˆã€Œå¤§å­¦ã€ã€Œå…¬åœ’ã€ãªã©ï¼‰ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚
        # ä»Šå›ã¯ä¸€æ—¦ãã®ã¾ã¾ã€‚ãŸã ã—ã€Œé§…ã€ãŒã¤ã„ã¦ã„ãªã„ã¨èª¤çˆ†ã—ã‚„ã™ã„ï¼ˆä¾‹: "æœ¬ç”º"ï¼‰
        if len(station) < 2: 
            continue
            
        # å®‰å…¨ç­–: "é§…" ãŒå¾Œã‚ã«ã¤ã„ã¦ã„ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯ç‰¹å®šã®ä¸»è¦é§…ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã‹
        # location = "äº”åç”°é§…å¾’æ­©5åˆ†" -> hit "äº”åç”°"
        if station in location:
            possible_prefs.update(prefs)

    if not possible_prefs:
        return None
        
    # å€™è£œã®ä¸­ã‹ã‚‰å„ªå…ˆé †ä½ã®é«˜ã„ã‚‚ã®ã‚’è¿”ã™
    for p in PREF_PRIORITY:
        if p in possible_prefs:
            return p
            
    # å„ªå…ˆãƒªã‚¹ãƒˆã«ãªã‘ã‚Œã°ã€å€™è£œã®æœ€åˆã®ä¸€ã¤ã‚’è¿”ã™
    return list(possible_prefs)[0]


def is_valid_job(job):
    # ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡
    if job.get('source') not in FILTER_TARGET_SOURCES:
        return True
        
    title = str(job.get('title', ''))
    summary = str(job.get('summary', ''))
    full_text = (title + summary).replace("\n", "").replace(" ", "")
    
    # 1. å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ (ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ«ã‚¿)
    if not any(req in full_text for req in REQUIRED_KEYWORDS):
        return False
        
    # 2. NGã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ (ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ«ã‚¿)
    if any(ng in full_text for ng in NG_KEYWORDS):
        return False
        
    return True

def main():
    # é§…ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    load_station_data()

    combined_jobs = []
    
    print("ğŸš€ Merging Job Data with Filters & Normalization...")
    
    for source, pattern in PROJECT_PATHS.items():
        latest_file = get_latest_file(pattern)
        if latest_file:
            print(f"âœ… Found {source}: {latest_file}")
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    jobs = data.get('jobs', []) if isinstance(data, dict) else data
                    
                    count_before = len(jobs)
                    valid_jobs = []
                    
                    for job in jobs:
                        if 'source' not in job:
                            job['source'] = source
                        
                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if is_valid_job(job):
                            # éƒ½é“åºœçœŒè£œå®Œãƒ­ã‚¸ãƒƒã‚¯
                            loc = job.get('location', '')
                            detected_pref = detect_prefecture(loc)
                            
                            if detected_pref:
                                job['prefecture'] = detected_pref
                                # locationã«éƒ½é“åºœçœŒãŒå«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°å…ˆé ­ã«ä»˜ä¸
                                if detected_pref not in loc:
                                    job['location'] = f"{detected_pref} {loc}"
                            
                            valid_jobs.append(job)
                            
                    combined_jobs.extend(valid_jobs)
                    
                    filtered_count = count_before - len(valid_jobs)
                    print(f"   -> Added {len(valid_jobs)} jobs (Filtered out {filtered_count} noise jobs)")
                    
            except Exception as e:
                print(f"   âŒ Error reading {latest_file}: {e}")
        else:
            print(f"âš ï¸ No data found for {source}")

    # é‡è¤‡æ’é™¤ï¼ˆå¿µã®ãŸã‚ãƒªãƒ³ã‚¯ã‚’ã‚­ãƒ¼ã«ï¼‰
    unique_jobs = {job['link']: job for job in combined_jobs if job.get('link')}
    final_list = list(unique_jobs.values())

    # IDä»˜ä¸ (ãƒªãƒ³ã‚¯ã®ãƒãƒƒã‚·ãƒ¥å€¤)
    for job in final_list:
        if 'link' in job:
            job['id'] = hashlib.md5(job['link'].encode('utf-8')).hexdigest()
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_list, f, indent=2, ensure_ascii=False)
        
    print("\n" + "="*30)
    print(f"ğŸ‰ Successfully merged {len(final_list)} jobs into {OUTPUT_FILE}")
    print("="*30)

if __name__ == "__main__":
    main()
